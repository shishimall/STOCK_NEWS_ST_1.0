# stock_news_st

# Excelã‚¨ã‚¤ãƒªã‚¢ã‚¹ + ãƒ‹ãƒ¥ãƒ¼ã‚¹ + Altairãƒãƒ£ãƒ¼ãƒˆ + é…å½“(TTM) + æ ªä¸»å„ªå¾…ãƒªãƒ³ã‚¯ + ã‚¿ã‚¤ãƒˆãƒ«æ—¥æœ¬èªç¤¾åï¼ˆã‚³ãƒ¼ãƒ‰ï¼‰

import os
import io
import re
import unicodedata
from pathlib import Path

import pandas as pd
import streamlit as st
import yfinance as yf
import feedparser  # type: ignore
import urllib.parse
import altair as alt

# ================= ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ =================
def _norm(s: str) -> str:
    return unicodedata.normalize("NFKC", str(s)).strip()

def _has_japanese(s: str) -> bool:
    return bool(re.search(r"[ã-ã‚“ã‚¡-ãƒ³ä¸€-é¾¥ï½¦-ï¾Ÿãƒ»ï½°ãƒ¼]", s))

DATA_DIR = Path("data")
ALIAS_PATH = DATA_DIR / "aliases.xlsx"

def _ensure_dir(p: Path):
    p.parent.mkdir(parents=True, exist_ok=True)

def _validate_alias_df(df: pd.DataFrame) -> pd.DataFrame:
    df = df.rename(columns={"Ticker": "ticker", "Alias": "alias"})
    if not {"ticker", "alias"}.issubset(df.columns):
        raise ValueError("å¿…è¦åˆ—ãŒã‚ã‚Šã¾ã›ã‚“ã€‚'ticker','alias' åˆ—ã‚’ç”¨æ„ã—ã¦ãã ã•ã„ã€‚")
    df = df[["ticker", "alias"]].copy()
    df["ticker"] = df["ticker"].map(_norm)
    df["alias"] = df["alias"].map(_norm)
    df = df.dropna(subset=["ticker", "alias"])
    return df

@st.cache_data(show_spinner=False)
def load_alias_from_disk() -> pd.DataFrame:
    if ALIAS_PATH.exists():
        if ALIAS_PATH.suffix.lower() in (".xlsx", ".xls"):
            df = pd.read_excel(ALIAS_PATH)
        elif ALIAS_PATH.suffix.lower() in (".csv", ".txt"):
            df = pd.read_csv(ALIAS_PATH)
        else:
            return pd.DataFrame(columns=["ticker", "alias"])
        return _validate_alias_df(df)
    return pd.DataFrame(columns=["ticker", "alias"])

def save_uploaded_alias(uploaded_file) -> str:
    fname = uploaded_file.name.lower()
    if fname.endswith((".xlsx", ".xls")):
        df = pd.read_excel(uploaded_file)
        ext = ".xlsx"
    elif fname.endswith((".csv", ".txt")):
        df = pd.read_csv(uploaded_file)
        ext = ".csv"
    else:
        raise ValueError("å¯¾å¿œæ‹¡å¼µå­ã¯ .xlsx/.xls/.csv/.txt ã§ã™ã€‚")

    df = _validate_alias_df(df)
    _ensure_dir(ALIAS_PATH)
    target = ALIAS_PATH.with_suffix(ext)
    tmp = target.with_suffix(ext + ".tmp")

    if ext == ".xlsx":
        with pd.ExcelWriter(tmp, engine="xlsxwriter") as w:
            df.to_excel(w, index=False)
    else:
        df.to_csv(tmp, index=False, encoding="utf-8-sig")

    if target.exists():
        target.unlink()
    tmp.rename(target)

    # æ—§å½¢å¼æƒé™¤
    for other in (".xlsx", ".xls", ".csv", ".txt"):
        p = ALIAS_PATH.with_suffix(other)
        if p.exists() and p != target:
            try:
                p.unlink()
            except Exception:
                pass

    load_alias_from_disk.clear()
    return str(target)

def download_current_alias_button(df: pd.DataFrame):
    buf = io.BytesIO()
    with pd.ExcelWriter(buf, engine="xlsxwriter") as w:
        (df if not df.empty else pd.DataFrame(columns=["ticker", "alias"])).to_excel(w, index=False)
    st.download_button(
        "ç¾åœ¨ã®ã‚¨ã‚¤ãƒªã‚¢ã‚¹è¡¨ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆxlsxï¼‰",
        data=buf.getvalue(),
        file_name="aliases.xlsx",
        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
        use_container_width=True,
    )

# ================= ã‚¿ã‚¤ãƒˆãƒ«ç”¨ï¼šæ—¥æœ¬èªç¤¾åã‚’é¸ã¶ =================
def display_name_for(code: str, alias_df: pd.DataFrame | None, info: dict | None) -> str:
    code = _norm(code)
    # 1) ã‚¨ã‚¤ãƒªã‚¢ã‚¹ã‹ã‚‰æ—¥æœ¬èªã‚‰ã—ã„ã‚‚ã®ã‚’æœ€å„ªå…ˆ
    if alias_df is not None and not alias_df.empty:
        cand = alias_df.loc[alias_df["ticker"] == code, "alias"].tolist()
        jp = [a for a in cand if _has_japanese(a)]
        if jp:
            return max(jp, key=len)
    # 2) yfinanceã®ç¤¾å
    if info:
        for k in ("longName", "shortName"):
            v = info.get(k)
            if v:
                return str(v)
    # 3) ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šã‚³ãƒ¼ãƒ‰
    return code

# ================= ãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾— =================
def _aliases_for(code: str, alias_df: pd.DataFrame | None = None):
    code = _norm(code)
    aliases = {code, code.replace(".T", "")}
    info = {}
    try:
        t = yf.Ticker(code)
        try:
            info = t.get_info()
        except Exception:
            pass
        for k in ("longName", "shortName"):
            v = info.get(k)
            if v:
                aliases.add(_norm(v))
    except Exception:
        pass
    if alias_df is not None and not alias_df.empty:
        extra = alias_df.loc[alias_df["ticker"] == code, "alias"].tolist()
        for a in extra:
            if a:
                aliases.add(_norm(a))
    manual = {"7611.T": ["ãƒã‚¤ãƒ‡ã‚¤æ—¥é«˜", "æ—¥é«˜å±‹"], "5020.T": ["ï¼¥ï¼®ï¼¥ï¼¯ï¼³", "ENEOS"]}
    for v in manual.get(code, []):
        aliases.add(_norm(v))
    return [a for a in aliases if a]

def _score_title(title: str, terms: list[str], code: str) -> int:
    t = _norm(title).lower()
    score = 0
    for a in terms:
        a_norm = _norm(a).lower()
        if a_norm and a_norm in t:
            score += 2
    core = code.replace(".T", "")
    if re.search(rf"(?:\(|ï¼ˆ|ã€){core}(?:\)|ï¼‰|ã€‘)", t):
        score += 2
    if core in t:
        score += 1
    return score

_EXCLUDE_TERMS = ["ã‚²ãƒ¼ãƒ ", "ã‚¹ãƒ—ãƒ©", "splatoon", "ã‚®ã‚¢", "ãƒ•ã‚§ã‚¹", "OCEANS", "ã‚ªãƒ¼ã‚·ãƒ£ãƒ³ã‚º"]

def fetch_news_for(code: str, alias_df: pd.DataFrame | None, days: int = 30, max_items: int = 10, strict_title=True, min_score=2):
    terms = _aliases_for(code, alias_df=alias_df)
    quoted_terms = [f'"{t}"' for t in terms]
    must_have = "(æ ªä¾¡ OR æ±ºç®— OR IR OR æ¥­ç¸¾ OR å‡ºåº— OR æ—¢å­˜åº— OR æœˆæ¬¡ OR å£²ä¸Š)"
    exclude = "-ã‚²ãƒ¼ãƒ  -ã‚¹ãƒ—ãƒ© -Splatoon -ã‚®ã‚¢ -ä»»å¤©å ‚ -eã‚¹ãƒãƒ¼ãƒ„ -ãƒ•ã‚§ã‚¹ -OCEANS"
    when = f"when:{days}d"
    q = f'({" OR ".join(quoted_terms)}) {must_have} {exclude} {when}'

    url = "https://news.google.com/rss/search?" + urllib.parse.urlencode({
        "q": q, "hl": "ja", "gl": "JP", "ceid": "JP:ja",
    })
    feed = feedparser.parse(url)

    pool = []
    for e in feed.entries[: max_items * 3]:
        title = getattr(e, "title", "")
        if not title:
            continue
        low = title.lower()
        if any(x.lower() in low for x in _EXCLUDE_TERMS):
            continue
        score = _score_title(title, terms, code)
        if (not strict_title) or (score >= min_score):
            pool.append({
                "title": title,
                "link": getattr(e, "link", ""),
                "published": getattr(e, "published", ""),
                "score": score,
            })

    pool.sort(key=lambda x: (x["score"], x.get("published", "")), reverse=True)
    return pool[:max_items]

# ================= é…å½“ï¼ˆTTM + ç›´è¿‘ï¼‰ =================
def get_dividend_info(code: str, last_close: float) -> dict:
    out = {"ttm_div": None, "yield_pct": None, "recent": pd.DataFrame()}
    try:
        ticker = yf.Ticker(code)
        s = ticker.dividends
        if s is None or len(s) == 0 or float(getattr(s, "sum", lambda: 0)()) == 0:
            df_dl = yf.download(code, period="5y", interval="1d", actions=True, auto_adjust=False, progress=False)
            if isinstance(df_dl, pd.DataFrame) and "Dividends" in df_dl.columns:
                s = df_dl["Dividends"]
            else:
                s = pd.Series(dtype="float64")
        if s is None or len(s) == 0:
            return out
        s = s[s > 0]
        if len(s) == 0:
            return out
        if getattr(s.index, "tz", None) is not None:
            s.index = s.index.tz_convert(None)
        cutoff = pd.Timestamp.utcnow() - pd.Timedelta(days=365)
        cutoff = cutoff.tz_localize(None)
        ttm = s[s.index >= cutoff]
        ttm_sum = float(ttm.sum()) if len(ttm) else 0.0
        out["ttm_div"] = ttm_sum if ttm_sum > 0 else None
        if out["ttm_div"] and last_close:
            out["yield_pct"] = out["ttm_div"] / float(last_close) * 100.0
        recent = s.sort_index(ascending=False).head(5).reset_index()
        recent.columns = ["date", "dividend"]
        out["recent"] = recent
    except Exception:
        pass
    return out

# ================= UI =================
st.set_page_config(page_title="éŠ˜æŸ„ãƒ€ãƒƒã‚·ãƒ¥ - ãƒ—ãƒ­ãƒˆ", layout="wide")
st.title("ğŸ“ˆ éŠ˜æŸ„ãƒ¯ãƒ³ã‚¯ãƒªãƒƒã‚¯è¦ç´„")

# --- ã‚µã‚¤ãƒ‰ãƒãƒ¼: ã‚¨ã‚¤ãƒªã‚¢ã‚¹ç®¡ç† ---
st.sidebar.markdown("### ã‚¨ã‚¤ãƒªã‚¢ã‚¹è¡¨ï¼ˆExcel/CSVï¼‰")
alias_df = load_alias_from_disk()
if ALIAS_PATH.exists():
    st.sidebar.caption(f"ä¿å­˜å…ˆ: `{ALIAS_PATH}`")
else:
    st.sidebar.caption("ä¿å­˜å…ˆ: ãªã—ï¼ˆåˆå›ã¯ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ä¿å­˜ï¼‰")

uploaded = st.sidebar.file_uploader("ã‚¨ã‚¤ãƒªã‚¢ã‚¹è¡¨ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼ˆxlsx/csv/txtï¼‰", type=["xlsx", "xls", "csv", "txt"])
c1, c2 = st.sidebar.columns(2)
with c1:
    if uploaded is not None and st.button("ä¿å­˜/å·®ã—æ›¿ãˆ", use_container_width=True):
        try:
            path = save_uploaded_alias(uploaded)
            st.sidebar.success(f"ä¿å­˜ã—ã¾ã—ãŸ: {path}")
        except Exception as e:
            st.sidebar.error(f"ä¿å­˜å¤±æ•—: {e}")
with c2:
    if st.button("ç¾åœ¨ã®è¡¨ã‚’DL", use_container_width=True):
        st.session_state["dl_alias"] = True

if st.session_state.get("dl_alias"):
    st.info("ç¾åœ¨ã®ã‚¨ã‚¤ãƒªã‚¢ã‚¹è¡¨ï¼ˆxlsxï¼‰ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã§ãã¾ã™ã€‚")
    download_current_alias_button(alias_df)
    st.session_state["dl_alias"] = False

with st.expander("ã‚¨ã‚¤ãƒªã‚¢ã‚¹è¡¨ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼", expanded=False):
    st.dataframe(alias_df, use_container_width=True)

# --- ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ ---
st.sidebar.markdown("---")
period = st.sidebar.selectbox("æœŸé–“", ["1mo", "3mo", "6mo", "1y"], index=1)
interval = st.sidebar.selectbox("è¶³", ["1d", "1wk", "1mo"], index=0)
use_zero_base = st.sidebar.checkbox("Yè»¸ã‚’0ã‹ã‚‰ã«ã™ã‚‹", value=False)

code = st.text_input("è¨¼åˆ¸ã‚³ãƒ¼ãƒ‰ or ãƒ†ã‚£ãƒƒã‚«ãƒ¼ï¼ˆä¾‹ï¼š5020.T / 5108.T / 7611.T / AAPLï¼‰", "5108.T")

if st.button("ç”Ÿæˆ", type="primary"):
    if not code:
        st.warning("ã‚³ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
    else:
        ticker = yf.Ticker(code)
        df = ticker.history(period=period, interval=interval)

        info = {}
        try:
            info = ticker.get_info()
        except Exception:
            pass
        name = display_name_for(code, alias_df, info)
        title_text = f"{name}ï¼ˆ{code}ï¼‰"

        if not df.empty:
            st.subheader(f"{title_text} â€“ ãƒãƒ£ãƒ¼ãƒˆ")
            chart_df = df.reset_index().rename(columns={"Date": "date"})
            ymin, ymax = float(df["Close"].min()), float(df["Close"].max())
            if use_zero_base:
                y_scale = alt.Scale(domain=[0, ymax * 1.05])
            else:
                y_scale = alt.Scale(domain=[ymin * 0.98, ymax * 1.02])
            line = (
                alt.Chart(chart_df)
                .mark_line()
                .encode(
                    x=alt.X("date:T", axis=alt.Axis(title=None)),
                    y=alt.Y("Close:Q", axis=alt.Axis(title=None), scale=y_scale),
                    tooltip=[
                        alt.Tooltip("date:T", title="æ—¥ä»˜"),
                        alt.Tooltip("Close:Q", title="çµ‚å€¤", format=".2f"),
                        alt.Tooltip("Open:Q",  title="å§‹å€¤", format=".2f"),
                        alt.Tooltip("High:Q",  title="é«˜å€¤", format=".2f"),
                        alt.Tooltip("Low:Q",   title="å®‰å€¤", format=".2f"),
                    ],
                )
                .properties(height=260)
                .interactive()
            )
            st.altair_chart(line, use_container_width=True)
            change = (df["Close"][-1] - df["Close"][0]) / df["Close"][0] * 100
            st.caption(f"æœŸé–“: {len(df)}æœ¬, å§‹å€¤={df['Open'][0]:.2f}, çµ‚å€¤={df['Close'][-1]:.2f}, å¤‰åŒ–ç‡={change:+.2f}%")
        else:
            st.info("ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

        # é…å½“
        st.subheader("ğŸ’´ é…å½“ï¼ˆç›´è¿‘1å¹´TTMï¼å‚è€ƒï¼‰")
        last_close = float(df["Close"][-1]) if not df.empty else None
        div_info = get_dividend_info(code, last_close if last_close else 0.0)
        if div_info["ttm_div"]:
            yld = f"{div_info['yield_pct']:.2f}%" if div_info["yield_pct"] is not None else "â€”"
            st.write(f"TTMé…å½“åˆè¨ˆ: **{div_info['ttm_div']:.2f}** / å‚è€ƒåˆ©å›ã‚Š: **{yld}**")
        else:
            st.write("é…å½“ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ï¼ˆéŠ˜æŸ„ã‚„æœŸé–“ã«ã‚ˆã£ã¦ã¯ yfinance ã‹ã‚‰å–å¾—ã§ããªã„ã“ã¨ãŒã‚ã‚Šã¾ã™ï¼‰")
        if not div_info["recent"].empty:
            st.dataframe(div_info["recent"].rename(columns={"date": "æ—¥ä»˜", "dividend": "é…å½“(1æ ª)"}), use_container_width=True)

        # ğŸ”— æ ªä¸»å„ªå¾…ãƒªãƒ³ã‚¯ã‚’è¿½åŠ 
        st.markdown(
            "ğŸ”— æ ªä¸»å„ªå¾…æƒ…å ±ã¯ä»¥ä¸‹ã®å¤–éƒ¨ã‚µã‚¤ãƒˆã§ç¢ºèªã§ãã¾ã™ï¼š\n"
            "[æ ªä¸»å„ªå¾…ã‚’æ¢ã™ï¼ˆkabuyutai.comï¼‰](https://www.kabuyutai.com/)"
        )

        # ãƒ‹ãƒ¥ãƒ¼ã‚¹
        st.subheader("ğŸ“° ãƒ‹ãƒ¥ãƒ¼ã‚¹ï¼ˆGoogle News RSSï¼‰")
        news = fetch_news_for(code, alias_df=alias_df, days=30, max_items=8, strict_title=True, min_score=2)
        if news:
            for n in news:
                st.markdown(f"- [{n['title']}]({n['link']}) ã€”score={n['score']}ã€•")
        else:
            st.write("ãƒ‹ãƒ¥ãƒ¼ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

st.caption("Powered by Streamlit / yfinance / Google News RSS")
